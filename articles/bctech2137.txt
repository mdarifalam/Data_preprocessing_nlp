Key Audit Matters Predictive Modeling

Client:A leading business school worldwide

Industry Type:R&D

Services:Research & Innovation

Organization Size:10000+

Do regression modeling on the data provided, cross-country determinants of Key Audit Matters (KAMs) and its usefulness to Investors and Debt Market Participants

USEFULNESS TO EQUITY MARKETS

In order to do the analysis and hypothesis testing, create a mapping to divide the audits into sub category and category according to the sub category and category provided in the question document. Clean the data before proceeding and calculate variables ABRET, ABVOL, CAR and CAAR according to the description provided.

Created a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.

Calculation of variable ABRET and ABVOL is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. Cleaning is done on the data by removing the repetitive entries from the dataset and then selected the data around the date for which the variable is to be calculated. Similarly, calculated ABVOL in which extracted the data around the annual report filing date and mean value for 40 days interval that ends 21 days before earning announcement dates.

Couldnâ€™t proceed because dataset provided by the client was incomplete in order to calculate ABRET.

R language to create mapping for the key audit matters and save data set for question 1.

Python pandas library to deal with dates and extract data around annual report filing date.

Data mapping, data cleaning, data manipulation, debugging

Key audit matter

GDP rule law

Audit fee

Trading data

Earning date

Report filing date

Dataset provided by the client was too big and made my system slow when the data is loaded in the environment. Too many datasets and variables made it bit difficult to understand and time taking.

Calculated the number of unique identifiers in the large dataset and sorted those. Then selected the data for 1 unique identifier and sorted dates for it and append it to the dataframe and saved group of such unique identifiers to reduce the size of the dataset and performed the calculations in loop.

To tackle the difficulty of understanding the data I made a document tracking all the columns or variables present in the data.