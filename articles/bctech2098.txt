Data integration and big data performance using Elasticsearch

Client:A Leading Tech Firm in the USA

Industry Type:IT & Consulting

Services:Software, Business Solutions, Consulting

Organization Size:200+

Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.

The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.

Elasticsearch

Postman

Kibana

Logstash

Python

Javascript

Amazon Web Services

Postgres

Docker

Git Bucket

Github

Javascript

Json

Domain-Specific Language for elasticsearch

bash

Elasticsearch query knowledge

Postgres query knowledge

Networking

Javascript

Backend web stack

Postgres

Elasticsearch

Amazon Web Services (AWS)

To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.

Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.